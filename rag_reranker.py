import os
import shutil
os.environ['USER_AGENT'] = 'myagent'
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from loader import loader_docs
from langchain_ollama import ChatOllama
# New import for the reranker
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
import pandas as pd


#load huggingface token from .env
def load_token():
    # It's better to load this from an environment variable for security
    # For example: os.environ.get("HUGGINGFACEHUB_API_TOKEN")
    api_key = "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"
    if not api_key:
        raise ValueError("HUGGINGFACEHUB_API_TOKEN is not set in the environment variables.")
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = api_key
    return api_key


#load the document
# This function is called but its return value isn't used in main.
# Assuming it's for pre-loading or caching.
loader_docs()

#splitting the texts
def split_docs(docs):
    text_splitter = RecursiveCharacterTextSplitter(
            chunk_size = 1024,
            chunk_overlap = 100,
            separators= ["\n### ", "\n#### ", "\n", " ", ""]
    )
    return text_splitter.split_documents(docs)



#creating vectorstore using huggingface embedding model
def create_vectorstore(chunks, save_path="vect_store"):
    if os.path.exists(save_path):
        shutil.rmtree(save_path)
        print(f'removed the old vectorstore at {save_path}')

    embeddings_model = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")
    vectorstore = FAISS.from_documents(documents= chunks, embedding= embeddings_model)
    vectorstore.save_local(save_path)
    print(f'new vectorstore saved at {save_path}')

    return vectorstore



def create_llm_chain():
    llm = ChatOllama(
        model="llama3.1",
        temperature=0
    )

    chat_model = llm
    system_prompt = """
        ## ROLE ##
        You are an expert Q&A assistant for Chameleon Cloud, a testbed for computer science research.

        ## TASK ##
        Your primary goal is to provide a comprehensive and helpful answer by synthesizing information from ALL relevant context sources provided. You must accurately interpret the user's intent to deliver the most useful response.

        ## INSTRUCTIONS ##
        - First, understand the user's question to determine their underlying intent (e.g., are they asking for a definition, a step-by-step guide, or troubleshooting help?).
        - Scrutinize all provided context sources to gather relevant information.
        - Synthesize a single, cohesive answer from the different sources. Do not simply list information from each source separately.
        - If the answer is not present in the context, you MUST respond with the single phrase: "I don't know."
        - Do not use any information outside of the provided context. Do not make up answers.
        - After your answer, list all the sources you used to construct it. Be explict about the source citation, including the URL and the source title. These must be included in the list of sources.

        ## OUTPUT FORMAT ##
        <A comprehensive, synthesized answer that directly addresses the user's intent.>

        ---
        ### Read More:
        * **[Title of Source 1]:** <URL from metadata>
        * **[Title of Source 2]:** <URL from metadata>
        * **[Title of Source n]:** <URL from metadata>

        ## CONTEXT ##

        {context}
    """

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", "Question:{question}")
    ])

    return prompt | chat_model

def create_llm_evaluator():
    llm = ChatOllama(
        model="llama3.1",
        temperature=0
    )

    chat_model = llm
    system_prompt = """
        ROLE
        You are an Expert Editor and Quality Grader for a Q&A assistant specializing in Chameleon Cloud. You do not answer questions yourself; you review and perfect answers generated by another AI.

        TASK
        Your task is to evaluate, correct, and refine an answer based on a strict set of rules. You will be given the original user's question, the same context sources the first AI used, and the answer it generated. Your goal is to ensure the final output is a perfect, comprehensive, and context-grounded response.

        INSTRUCTIONS
        Review the provided answer against the original context using the following checklist. Your output should be the refined answer, not your evaluation notes.

        EVALUATION & REFINEMENT CHECKLIST:

        Verify Factual Accuracy: Is every statement in the answer directly supported by the provided context?

        Action: Remove any information that is not present in the sources (i.e., "hallucinated" content).

        Check for Completeness: Does the answer synthesize information from ALL relevant parts of the context to fully address the user's question?

        Action: If the original answer missed relevant details from the context, integrate them into a single, cohesive response.

        Ensure Proper Synthesis: Is the answer a well-written, synthesized response, or is it just a list of separate facts from the sources?

        Action: Rewrite the answer to ensure it flows logically and reads as a single, comprehensive explanation.

        Validate "I don't know": If the first model provided an answer, but the information was not actually in the context, was the correct response "I don't know"?

        Action: If the context does not contain the answer, replace the entire generated answer with the single phrase: I don't know.

        Correct Source Citation: Does the answer include a "Read More" section that correctly lists the sources used, including the title and URL from the metadata?

        Action: Add or correct the source list to match the required format exactly.

        FINAL OUTPUT FORMAT
        After your review and refinement, your final output must be the improved answer only and strictly follow this format:

        <The corrected and comprehensive, synthesized answer.>

        Read More:
        [Title of Source 1]: <URL from metadata>

        [Title of Source 2]: <URL from metadata>

        ...

        [Title of Source n]: <URL from metadata>

        CONTEXT

        {context}
    """

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", "Question:{answer}")
    ])

    return prompt | chat_model


def evaluate_from_csv(chain, evaluator, retriever, file_path="./questions.csv", new_column_name='Answer'):
    """
    Reads questions from a CSV file, gets model answers, and writes them back to a new column.
    """
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f"Error: The file {file_path} was not found.")
        df = pd.DataFrame(columns=['Query'], data=[
            """What is the purpose of the Chameleon Associate Sites?""",
            """What specific model of GPU is available on the gpu_rtx_8000 nodes?""",
            """How do I create an isolated network for my instances using the GUI?""",
            """How do I find and use a pre-configured Jupyter notebook on Chameleon?""",
            """My bare metal node is stuck in the "deploying" state for a long time. What should I do?""",
            """I get an "Error 403: Forbidden" when trying to use the OpenStack CLI. What's wrong?""",
            """What is "CHI-in-a-Box"?""",
            """What does it mean for Chameleon to support "deep reconfigurability"?""",
            """What is the difference between a Floating IP and a Private IP?""",
            """When should I use an FPGA node versus a GPU node?""",
            """How do I cite Chameleon in my research paper?""",
            """How do I attach a volume to my running instance?""",
            """What is the maximum duration for a single lease reservation?""",
            """Can I use my Chameleon allocation to mine cryptocurrency?""",
            """How can I set up an experiment that requires a specific kernel version on a bare metal node?""",
            """What is the Trovi artifacts repository?""",
            """How do I launch multiple nodes with one command using Heat templates?""",
            """Where are the main Chameleon hardware sites located?""",
            """Why can't my instance access the internet, even with a floating IP?""",
            """How can I repeat a networking experiment I found on the Chameleon blog to ensure reproducibility?"""
        ])

    answers = []
    print("Processing queries from the CSV file...")
    for index, row in df.iterrows():
        query = row['Query']
        print(f"Processing query {index + 1}: {query}")

        instructional_query = f"A question regarding the Chameleon Cloud testbed: {query}"
        retrieved_docs = retriever.invoke(instructional_query)
        context = "\n\n".join(doc.page_content for doc in retrieved_docs)
        response = chain.invoke({"context": context, "question": query}).content
        improved_response = evaluator.invoke({"context": context, "answer": response}).content
        
        print("\nResponse:")
        print(improved_response)
        answers.append(improved_response)

    df[new_column_name] = answers
    df.to_csv(file_path, index=False)
    print(f"\nProcessing complete. The answers have been added to the '{new_column_name}' column in {file_path}")


def main():
    load_token()
    docs = loader_docs()
    chunks = split_docs(docs)
    print(f"Number of chunks: {len(chunks)}")
    vectorstore = create_vectorstore(chunks)

    # Standard retriever
    retriever = vectorstore.as_retriever(search_kwargs={'k': 20}) # Retrieve more documents initially

    # Initialize the reranker model
    model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-large")
    compressor = CrossEncoderReranker(model=model, top_n=10) # Rerank and get top 10

    # Create the compression retriever
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor, base_retriever=retriever
    )

    chain = create_llm_chain()
    evaluator = create_llm_evaluator()

    evaluate_from_csv(chain, evaluator, compression_retriever, file_path="./questions.csv", new_column_name='Answer')

    print("________________________________________________________________________________________________________________")
    print("Type 'exit' to quit.")
    while True:
        query = input("\n Please enter a question: ")
        if query == "exit":
            break
        
        # The instructional query is a good idea for improving retrieval
        instructional_query = f"A question regarding the Chameleon Cloud testbed: {query}"
        
        # Use the compression retriever to get reranked documents
        retrieved_docs = compression_retriever.invoke(instructional_query)
        
        context = "\n\n".join(doc.page_content for doc in retrieved_docs)
        
        # Pass the context to the chain
        response = chain.invoke({"context": context, "question": query}).content
        improved_response = evaluator.invoke({"context": context, "answer": response}).content
        
        print("\nResponse:")
        print(improved_response)

        print("\n\n--- Retrieved Documents ---")
        for i, doc in enumerate(retrieved_docs):
            print(f"\n================== Match {i+1} (Score: {doc.metadata.get('relevance_score', 'N/A')}) ===================")
            print(doc.page_content)
            print(f"Metadata: {doc.metadata}")

if __name__ == "__main__":
    main()
